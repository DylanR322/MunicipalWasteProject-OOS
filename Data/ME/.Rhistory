dta <- import("31117492.dta")
# now we'll create a variable for trump approval
dta$tapp <- NA
dta$tapp[dta$trumpapp == 1 | dta$trumpapp == 2] <- 1
dta$tapp[dta$trumpapp == 3 | dta$trumpapp == 4] <- 0
# now to look at the rsex, RECAGE5, and racethn variables with table and attributes, then clean them a little by at least putting down NA's
attributes(dta$rsex)
table(dta$rsex)
dta$rsex[dta$rsex == 8 | dta$rsex == 9] <- NA
attributes(dta$RECAGE5)
table(dta$RECAGE5)
dta$RECAGE5[dta$RECAGE5 == 9] <- NA
attributes(dta$racethn)
table(dta$racethn)
dta$racethn[dta$racethn == 9] <- NA
reg <- lm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(reg)
install.packages("sandwich")
install.packages("lmtest")
coeftest(reg, vcov = vcovHC(reg, type="HC3"))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(haven)
library(tidyr)
library(stringr)
library(stargazer)
library(ggplot2)
library(rio)
library(sandwich)
library(lmtest)
rm(list = ls())
knitr::opts_knit$set(root.dir = "~/Dropbox/PSCI338/PSETS/PSET 7/")
# Let's load up the data
dta <- import("31117492.dta")
# now we'll create a variable for trump approval
dta$tapp <- NA
dta$tapp[dta$trumpapp == 1 | dta$trumpapp == 2] <- 1
dta$tapp[dta$trumpapp == 3 | dta$trumpapp == 4] <- 0
# now to look at the rsex, RECAGE5, and racethn variables with table and attributes, then clean them a little by at least putting down NA's
attributes(dta$rsex)
table(dta$rsex)
dta$rsex[dta$rsex == 8 | dta$rsex == 9] <- NA
attributes(dta$RECAGE5)
table(dta$RECAGE5)
dta$RECAGE5[dta$RECAGE5 == 9] <- NA
attributes(dta$racethn)
table(dta$racethn)
dta$racethn[dta$racethn == 9] <- NA
reg <- lm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(reg)
coeftest(reg, vcov = vcovHC(reg, type="HC3"))
# now let's do a quick regression on tapp and the variables we just looked at.
reg <- lm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(reg)
# now let's estimate standard errors with the sandwich library
coeftest(reg, vcov = vcovHC(reg, type="HC3"))
dta$predictols <- predict(reg)
names(dta)
trump <- na.omit(dta[, c(1, 207, 125, 133, 160)])
trump$predictols <- predict(reg)
View(trump)
reg2 <- lm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = trump)
trump$predictols2 <- predict(reg2)
View(trump)
View(trump)
View(trump)
table(dta$tapp)
probit <- glm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), family = binomial(link = "probit"), data = dta)
summary(probit)
trump$predictprob <- predict(probit)
trump$diffolsprob <- trump$predictprob - trump$predictols
# We can use the predict function to get predicted values
dat$predictedvalues_probit <- predict(output2, type = "response")
pacman::p_load(foreign, stargazer, margins, effects, haven, car, lmtest, sandwich, dplyr, tidyverse)
dat <- read_dta("BidenTrump.dta")
# code whether or not someone got the answer correct
dat$correctanswer <- 0
dat$correctanswer[dat$answer == dat$response] <- 1
# Recode section and better as factors
dat$section.f <- as.factor(dat$section)
dat$better <- as.factor(dat$better)
# Look at some basic descriptives
stargazer(data.frame(dat), type = "text")
output1 <- lm(correctanswer ~ section.f + margin + better, data = dat)
summary(output1)
# Let's calculate predicted values for each observation
dat$predictedvalues_ols <- predict(output1)
summary(dat$predictedvalues_ols)
# Are there values below 0?
belowzero <- subset(dat, predictedvalues_ols < 0)
output2 <- glm(correctanswer ~ section.f + margin + better, family=binomial(link="probit"),
data = dat)
summary(output2)
# We can use the predict function to get predicted values
dat$predictedvalues_probit <- predict(output2, type = "response")
print(dat[1, c("predictedvalues_ols", "predictedvalues_probit")])
# Value of the latent quality of student 1's guesses
intercept <- output2$coefficients[1]
section.2 <- output2$coefficients[2]
section.3 <- output2$coefficients[3]
margin <- output2$coefficients[4]
better <- output2$coefficients[5]
latent <- intercept+section.2*0+section.3*0+margin*dat$margin[1]+better*1
attributes(latent) <- NULL
print(latent)
# This is the latent value of Y (called Y*). We want the outcome of this latent preference, or the expected probability that this student guesses correctly.
# Thus, we get our predicted probability by calculating the probability that a std. normal r.v. takes on a value of 0.4682398 or more
print(pnorm(latent))
# We see that we ensure we get a positive probabillity for a case that generated a negative probability using OLS
print(pnorm(-latent))
# Notice that the predicted probability is not the same when our latent variable is negative versus when it is positive. This is a function of probabilities (which must range from 0 to 1), and this is one of the reasons why we use probit or logit models to estimate binary outcomes (because our outcome must fall between 0 and 1)
probit <- glm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), family = binomial(link = "probit"), data = trump)
summary(probit)
trump$predictprob <- predict(probit)
trump$diffolsprob <- trump$predictprob - trump$predictols
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(haven)
library(tidyr)
library(stringr)
library(stargazer)
library(ggplot2)
library(rio)
library(sandwich)
library(lmtest)
rm(list = ls())
knitr::opts_knit$set(root.dir = "~/Dropbox/PSCI338/PSETS/PSET 7/")
# Let's load up the data
dta <- import("31117492.dta")
# now we'll create a variable for trump approval
dta$tapp <- NA
dta$tapp[dta$trumpapp == 1 | dta$trumpapp == 2] <- 1
dta$tapp[dta$trumpapp == 3 | dta$trumpapp == 4] <- 0
# now to look at the rsex, RECAGE5, and racethn variables with table and attributes, then clean them a little by at least putting down NA's
attributes(dta$rsex)
table(dta$rsex)
dta$rsex[dta$rsex == 8 | dta$rsex == 9] <- NA
attributes(dta$RECAGE5)
table(dta$RECAGE5)
dta$RECAGE5[dta$RECAGE5 == 9] <- NA
attributes(dta$racethn)
table(dta$racethn)
dta$racethn[dta$racethn == 9] <- NA
# now let's do a quick regression on tapp and the variables we just looked at.
reg <- lm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(reg)
# now let's estimate standard errors with the sandwich library
coeftest(reg, vcov = vcovHC(reg, type="HC3"))
# now to create predicted probs of Trump approval, using the OLS model. First, let's make a dataset with no NA's and the vars we are using so that we can use predict
trump <- na.omit(dta[, c(1, 207, 125, 133, 160)])
trump$predictols <- predict(reg)
trump$predictprob <- predict(probit, type = 'response')
# now to run a similar model but as a probit
probit <- glm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), family = binomial(link = "probit"), data = trump)
summary(probit)
# now to construct predicted probabilities with the probit and have a look at how it differs to the ols
trump$predictprob <- predict(probit, type = 'response')
trump$diffolsprob <- trump$predictprob - trump$predictols
mean(trump$diffolsprob)
attributes(dta$trumpapp)
dta$trumappordered <- NA
dta$trumpappordered <- NA
attributes(dta$trumpapp)
dta$trumpappordered[dta$trumpapp == 1] <- 4
dta$trumpappordered[dta$trumpapp == 2] <- 3
dta$trumpappordered[dta$trumpapp == 3] <- 2
dta$trumpappordered[dta$trumpapp == 4] <- 1
regord <- lm(trumpappordered ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(regord)
install.packages("MASS")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(haven)
library(tidyr)
library(stringr)
library(stargazer)
library(ggplot2)
library(rio)
library(sandwich)
library(lmtest)
library(MASS)
rm(list = ls())
knitr::opts_knit$set(root.dir = "~/Dropbox/PSCI338/PSETS/PSET 7/")
regord <- lm(as.factor(trumpappordered) ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(haven)
library(tidyr)
library(stringr)
library(stargazer)
library(ggplot2)
library(rio)
library(sandwich)
library(lmtest)
library(MASS)
rm(list = ls())
knitr::opts_knit$set(root.dir = "~/Dropbox/PSCI338/PSETS/PSET 7/")
# Let's load up the data
dta <- import("31117492.dta")
# now we'll create a variable for trump approval
dta$tapp <- NA
dta$tapp[dta$trumpapp == 1 | dta$trumpapp == 2] <- 1
dta$tapp[dta$trumpapp == 3 | dta$trumpapp == 4] <- 0
# now to look at the rsex, RECAGE5, and racethn variables with table and attributes, then clean them a little by at least putting down NA's
attributes(dta$rsex)
table(dta$rsex)
dta$rsex[dta$rsex == 8 | dta$rsex == 9] <- NA
attributes(dta$RECAGE5)
table(dta$RECAGE5)
dta$RECAGE5[dta$RECAGE5 == 9] <- NA
attributes(dta$racethn)
table(dta$racethn)
dta$racethn[dta$racethn == 9] <- NA
# now let's do a quick regression on tapp and the variables we just looked at.
reg <- lm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(reg)
# now let's estimate standard errors with the sandwich library
coeftest(reg, vcov = vcovHC(reg, type="HC3"))
# now to create predicted probs of Trump approval, using the OLS model. First, let's make a dataset with no NA's and the vars we are using so that we can use predict
trump <- na.omit(dta[, c(1, 207, 125, 133, 160)])
trump$predictols <- predict(reg)
# now to run a similar model but as a probit
probit <- glm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), family = binomial(link = "probit"), data = trump)
summary(probit)
# now to construct predicted probabilities with the probit and have a look at how it differs to the ols
trump$predictprob <- predict(probit, type = 'response')
trump$diffolsprob <- trump$predictprob - trump$predictols
mean(trump$diffolsprob)
# Now we'll try and do an ordered one
attributes(dta$trumpapp)
dta$trumpappordered <- NA
dta$trumpappordered[dta$trumpapp == 1] <- 4
dta$trumpappordered[dta$trumpapp == 2] <- 3
dta$trumpappordered[dta$trumpapp == 3] <- 2
dta$trumpappordered[dta$trumpapp == 4] <- 1
regord <- lm(as.factor(trumpappordered) ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(regord)
regord <- lm(trumpappordered ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(regord)
mprobit <- polr(trumpappordered ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), method = "probit", data = dta)
mprobit <- polr(trumpappordered ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), method = "probit", data = droplevels(dta))
dta <- droplevels(dta)
mprobit <- polr(trumpappordered ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), method = "probit", data = dta)
mprobit <- polr(as.factor(trumpappordered) ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), method = "probit", data = dta)
summary(mprobit)
trump$mprobitpred <- predict(mprobit, type = "probs")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(haven)
library(tidyr)
library(stringr)
library(stargazer)
library(ggplot2)
library(rio)
library(sandwich)
library(lmtest)
library(MASS)
rm(list = ls())
knitr::opts_knit$set(root.dir = "~/Dropbox/PSCI338/PSETS/PSET 7/")
# Let's load up the data
dta <- import("31117492.dta")
# now we'll create a variable for trump approval
dta$tapp <- NA
dta$tapp[dta$trumpapp == 1 | dta$trumpapp == 2] <- 1
dta$tapp[dta$trumpapp == 3 | dta$trumpapp == 4] <- 0
# now to look at the rsex, RECAGE5, and racethn variables with table and attributes, then clean them a little by at least putting down NA's
attributes(dta$rsex)
table(dta$rsex)
dta$rsex[dta$rsex == 8 | dta$rsex == 9] <- NA
attributes(dta$RECAGE5)
table(dta$RECAGE5)
dta$RECAGE5[dta$RECAGE5 == 9] <- NA
attributes(dta$racethn)
table(dta$racethn)
dta$racethn[dta$racethn == 9] <- NA
# now let's do a quick regression on tapp and the variables we just looked at.
reg <- lm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(reg)
# now let's estimate standard errors with the sandwich library
coeftest(reg, vcov = vcovHC(reg, type="HC3"))
# now to create predicted probs of Trump approval, using the OLS model. First, let's make a dataset with no NA's and the vars we are using so that we can use predict
dta$trumpappordered <- NA
dta$trumpappordered[dta$trumpapp == 1] <- 4
dta$trumpappordered[dta$trumpapp == 2] <- 3
dta$trumpappordered[dta$trumpapp == 3] <- 2
dta$trumpappordered[dta$trumpapp == 4] <- 1
trump <- na.omit(dta[, c(1, 207:208, 125, 133, 160)])
trump$predictols <- predict(reg)
# now to run a similar model but as a probit
probit <- glm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), family = binomial(link = "probit"), data = trump)
summary(probit)
# now to construct predicted probabilities with the probit and have a look at how it differs to the ols
trump$predictprob <- predict(probit, type = 'response')
trump$diffolsprob <- trump$predictprob - trump$predictols
mean(trump$diffolsprob)
# Now we'll try and do an ordered one
regord <- lm(trumpappordered ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(regord)
# now to do a ordered probit function
dta <- droplevels(dta)
mprobit <- polr(as.factor(trumpappordered) ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), method = "probit", data = dta)
summary(mprobit)
# now to create predicted probabilities again, however I'll have to tweak trump a bit
trump$mprobitpred <- predict(mprobit, type = "probs")
View(trump)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(haven)
library(tidyr)
library(stringr)
library(stargazer)
library(ggplot2)
library(rio)
library(sandwich)
library(lmtest)
library(MASS)
rm(list = ls())
knitr::opts_knit$set(root.dir = "~/Dropbox/PSCI338/PSETS/PSET 7/")
dta <- import("31117492.dta")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(haven)
library(tidyr)
library(stringr)
library(stargazer)
library(ggplot2)
library(rio)
library(sandwich)
library(lmtest)
library(MASS)
rm(list = ls())
knitr::opts_knit$set(root.dir = "~/Dropbox/PSCI338/PSETS/PSET 7/")
# first to import the data
dta <- import("31117492.dta")
# now we'll create a variable for trump approval
dta$tapp <- NA
dta$tapp[dta$trumpapp == 1 | dta$trumpapp == 2] <- 1
dta$tapp[dta$trumpapp == 3 | dta$trumpapp == 4] <- 0
# now to look at the rsex, RECAGE5, and racethn variables with table and attributes, then clean them a little by at least putting down NA's
attributes(dta$rsex)
table(dta$rsex)
dta$rsex[dta$rsex == 8 | dta$rsex == 9] <- NA
attributes(dta$RECAGE5)
table(dta$RECAGE5)
dta$RECAGE5[dta$RECAGE5 == 9] <- NA
attributes(dta$racethn)
table(dta$racethn)
dta$racethn[dta$racethn == 9] <- NA
# now let's do a quick regression on tapp and the variables we just looked at.
reg <- lm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), data = dta)
summary(reg)
# now let's estimate standard errors with the sandwich library
coeftest(reg, vcov = vcovHC(reg, type="HC3"))
# now to create predicted probs of Trump approval, using the OLS model. First, let's make a dataset with no NA's and the vars we are using so that we can use predict
dta$trumpappordered <- NA
dta$trumpappordered[dta$trumpapp == 1] <- 4
dta$trumpappordered[dta$trumpapp == 2] <- 3
dta$trumpappordered[dta$trumpapp == 3] <- 2
dta$trumpappordered[dta$trumpapp == 4] <- 1
trump <- na.omit(dta[, c(1, 207:208, 125, 133, 160)])
trump$predictols <- predict(reg)
# now to run a similar model but as a probit
probit <- glm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), family = binomial(link = "probit"), data = trump)
summary(probit)
# now to construct predicted probabilities with the probit and have a look at how it differs to the ols
trump$predictprob <- predict(probit, type = 'response')
View(trump)
trump$predictprob[, trump$rsex == 1 & trump$RECAGE5 == 2 & trump$racethn == 1]
trump$predictprob[trump$rsex == 1 & trump$RECAGE5 == 2 & trump$racethn == 1]
# first to import the data
dta <- import("31117492.dta")
# now we'll create a variable for trump approval
dta$tapp <- NA
dta$tapp[dta$trumpapp == 1 | dta$trumpapp == 2] <- 1
dta$tapp[dta$trumpapp == 3 | dta$trumpapp == 4] <- 0
# now to look at the rsex, RECAGE5, and racethn variables with table and attributes, then clean them a little by at least putting down NA's
attributes(dta$rsex)
table(dta$rsex)
dta$rsex[dta$rsex == 8 | dta$rsex == 9] <- NA
attributes(dta$RECAGE5)
table(dta$RECAGE5)
dta$RECAGE5[dta$RECAGE5 == 9] <- NA
attributes(dta$racethn)
table(dta$racethn)
dta$racethn[dta$racethn == 9] <- NA
# let's make a dataset with no NA's and the vars we are using so that we can use predict
trump <- na.omit(dta[, c(1, 207:208, 125, 133, 160)])
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(haven)
library(tidyr)
library(stringr)
library(stargazer)
library(ggplot2)
library(rio)
library(sandwich)
library(lmtest)
library(MASS)
rm(list = ls())
knitr::opts_knit$set(root.dir = "~/Dropbox/PSCI338/PSETS/PSET 7/")
# first to import the data
dta <- import("31117492.dta")
# now we'll create a variable for trump approval
dta$tapp <- NA
dta$tapp[dta$trumpapp == 1 | dta$trumpapp == 2] <- 1
dta$tapp[dta$trumpapp == 3 | dta$trumpapp == 4] <- 0
# now to look at the rsex, RECAGE5, and racethn variables with table and attributes, then clean them a little by at least putting down NA's
attributes(dta$rsex)
table(dta$rsex)
dta$rsex[dta$rsex == 8 | dta$rsex == 9] <- NA
attributes(dta$RECAGE5)
table(dta$RECAGE5)
dta$RECAGE5[dta$RECAGE5 == 9] <- NA
attributes(dta$racethn)
table(dta$racethn)
dta$racethn[dta$racethn == 9] <- NA
# let's make a dataset with no NA's and the vars we are using so that we can use predict
trump <- na.omit(dta[, c(1, 207, 125, 133, 160)])
# now to run a probit
probit <- glm(tapp ~ as.factor(rsex) + as.factor(RECAGE5) + as.factor(racethn), family = binomial(link = "probit"), data = trump)
summary(probit)
# now to construct predicted probabilities with the probit
trump$predictprob <- predict(probit, type = 'response')
# since we're looking for a white/non hispanic male between 30 and 44...
trump$predictprob[trump$rsex == 1 & trump$RECAGE5 == 2 & trump$racethn == 1]
library(rio)
snow <- import("~/Dropbox/Recitation1a.csv")
View(snow)
sd(snow$`Mean Dec-May snow depth (inches)`)
summary(snow$`Mean Dec-May snow depth (inches)`)
boxplot(snow$`Mean Dec-May snow depth (inches)`)
temp <- import("~/Dropbox/Recitation1b.csv")
View(temp)
summary(temp$Year)
t.test(mean(temp$`Temperature (degrees F)`)[temp$Year >= 1962 & temp$Year <= 2012] -
mean(temp$`Temperature (degrees F)`)[temp$Year >= 1880 & temp$Year <= 1930])
mean(temp$`Temperature (degrees F)`)[temp$Year >= 1962 & temp$Year <= 2012]
temp$`Temperature (degrees F)`)[temp$Year >= 1962 & temp$Year <= 2012]
t.test(mean(temp$`Temperature (degrees F)`[temp$Year >= 1962 & temp$Year <= 2012]) -
mean(temp$`Temperature (degrees F)`[temp$Year >= 1880 & temp$Year <= 1930]))
mean(temp$`Temperature (degrees F)`[temp$Year >= 1962 & temp$Year <= 2012])
mean(temp$`Temperature (degrees F)`[temp$Year >= 1880 & temp$Year <= 1930])
cor(temp$`Temperature (degrees F)`, temp$`[CO2] ppm`, method = 'pearson')
cor(temp$`Temperature (degrees F)`, temp$`[CO2] ppm`, method = 'pearson') * cor(temp$`Temperature (degrees F)`, temp$`[CO2] ppm`, method = 'pearson')
model <- lm(`Temperature (degrees F)` ~ `[CO2] ppm`, temp)
summary(model)
predict(model, 420)
predict(model, number)
number <- 420
predict(model, number)
summary(model)
library(rio)
snow <- import("~/Downloads/Expenses 81688.csv")
View(snow)
sum(snow$Amount)
sum(snow$Amount[snow$`Expense Type` == "Eating Out"])
sum(snow$Amount[snow$`Expense Type` == "Groceries"])
snow <- import("~/Downloads/Expenses 81688.csv")
View(snow)
sum(snow$Amount[snow$`Expense Type` == "Personal"])
#### OOS Waste Data Project
## Merging Data from Different States
## Dylan Radley
library(rio)
library(dplyr)
library(tidyr)
library(stringr)
library(readxl)
library(pdftools)
rm(list = ls())
setwd("~/Desktop/Trash Data Project OOS/Files/")
### MA + RI --------------------
ma <- import("ma.csv")
ri <- import("ri.csv")
dta <- bind_rows(ma, ri)
View(dta)
#### OOS Waste Data Project
## ME Municipal Data
## Dylan Radley
library(rio)
library(dplyr)
library(tidyr)
library(stringr)
library(readxl)
rm(list = ls())
setwd("/Users/dylanradley/Desktop/Trash Data Project OOS/Data/ME/")
txt <- pdf_text("Muni Reports 1/Alfred DEP Biennial Report 2019-2020.pdf") # read it in
page <- txt[1] # just take the first page with the table
page
rows <- scan(textConnection(page),
what = "character", sep = "\n") # break it into rows in the 'table'
rows2 <- strsplit(page, "\n")
rows2 <- rows2[[1]]
install.packages(tesseract)
install.packages("tesseract")
library(tesseract)
lapply("Muni Reports 1/Alfred DEP Biennial Report 2019-2020.pdf", function(i){
# convert pdf to jpef/tiff and perform tesseract OCR on the image
# Read in the PDF
pdf <- pdf_text(i)
# convert pdf to tiff
bitmap <- pdf_render_page(news, dpi = 300)
tiff::writeTIFF(bitmap, paste0(i, ".tiff"))
# perform OCR on the .tiff file
out <- ocr(paste0, (".tiff"))
# delete tiff file
file.remove(paste0(i, ".tiff" ))
})
install.packages("png")
library(png)
ex <- readPNG("Example.pdf")
ex <- writePNG("Example.pdf")
